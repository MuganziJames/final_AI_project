import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import pickle
import os
from typing import Dict, List, Tuple, Any
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class EnhancedDataPreprocessor:
    def __init__(self):
        self.scalers = {}
        self.encoders = {}
        self.feature_columns = {}
        self.data_quality_metrics = {}
        self.climate_data = None
        self.agriculture_data = None
        self.wb_climate_data = None
        
    def load_enhanced_datasets(self, data_dir: str):
        """Load all enhanced datasets for better predictions"""
        try:
            # Load climate change impact on agriculture (most comprehensive)
            climate_agri_path = os.path.join(data_dir, 'climate_change_impact_on_agriculture_2024.csv')
            if os.path.exists(climate_agri_path):
                self.agriculture_data = pd.read_csv(climate_agri_path)
                print(f"Loaded agriculture data: {len(self.agriculture_data)} records")
            
            # Load climate change indicators
            climate_indicators_path = os.path.join(data_dir, 'climate_change_indicators.csv')
            if os.path.exists(climate_indicators_path):
                self.climate_data = pd.read_csv(climate_indicators_path)
                print(f"Loaded climate indicators: {len(self.climate_data)} records")
                
            # Load World Bank climate data
            wb_path = os.path.join(data_dir, 'WB_Climatechange_data.csv')
            if os.path.exists(wb_path):
                self.wb_climate_data = pd.read_csv(wb_path)
                print(f"Loaded World Bank climate data: {len(self.wb_climate_data)} records")
                
        except Exception as e:
            print(f"Warning: Could not load enhanced datasets: {e}")
    
    def get_climate_features_for_location(self, location_data: Dict, year: int = 2024) -> Dict:
        """Extract climate features for a specific location and year"""
        features = {}
        country = location_data.get('country', '').lower()
        
        # Get temperature trends from climate indicators
        if self.climate_data is not None:
            country_climate = self.climate_data[
                self.climate_data['Country'].str.lower().str.contains(country, na=False)
            ]
            
            if not country_climate.empty:
                # Get recent temperature changes (last 5 years)
                recent_cols = [f'F{y}' for y in range(2018, 2023) if f'F{y}' in country_climate.columns]
                if recent_cols:
                    recent_temps = country_climate[recent_cols].iloc[0]
                    features['avg_temp_change'] = recent_temps.mean()
                    features['temp_trend'] = recent_temps.iloc[-1] - recent_temps.iloc[0]
                    features['temp_volatility'] = recent_temps.std()
        
        # Get agricultural context from agriculture data
        if self.agriculture_data is not None:
            country_agri = self.agriculture_data[
                self.agriculture_data['Country'].str.lower().str.contains(country, na=False)
            ]
            
            if not country_agri.empty:
                # Get recent agricultural indicators
                recent_agri = country_agri[country_agri['Year'] >= 2020]
                if not recent_agri.empty:
                    features['avg_temperature'] = recent_agri['Average_Temperature_C'].mean()
                    features['avg_precipitation'] = recent_agri['Total_Precipitation_mm'].mean()
                    features['co2_emissions'] = recent_agri['CO2_Emissions_MT'].mean()
                    features['extreme_weather_freq'] = recent_agri['Extreme_Weather_Events'].mean()
                    features['irrigation_access'] = recent_agri['Irrigation_Access_%'].mean()
                    features['soil_health'] = recent_agri['Soil_Health_Index'].mean()
                    features['crop_yield_avg'] = recent_agri['Crop_Yield_MT_per_HA'].mean()
        
        # Add geographical factors
        if 'lat' in location_data:
            features['latitude'] = location_data['lat']
            features['climate_zone'] = self._get_climate_zone(location_data['lat'])
        
        if 'lon' in location_data:
            features['longitude'] = location_data['lon']
            
        return features
    
    def _get_climate_zone(self, latitude: float) -> int:
        """Classify climate zone based on latitude"""
        abs_lat = abs(latitude)
        if abs_lat < 23.5:
            return 1  # Tropical
        elif abs_lat < 35:
            return 2  # Subtropical
        elif abs_lat < 50:
            return 3  # Temperate
        else:
            return 4  # Cold

# Keep the original class name for backward compatibility
class DataPreprocessor(EnhancedDataPreprocessor):
    
    def load_and_clean_data(self, file_path: str) -> pd.DataFrame:
        df = pd.read_csv(file_path)
        df = self._clean_dataset(df)
        return df
    
    def _clean_dataset(self, df: pd.DataFrame) -> pd.DataFrame:
        # Don't drop all NaN rows - only drop rows where ALL important columns are missing
        # For hunger data, we only need Entity, Year, and Global Hunger Index columns
        
        # Skip outlier removal for hunger data to preserve legitimate high-hunger countries
        # The annotation column has many NaN values but is not needed for training
        return df.reset_index(drop=True)
    
    def prepare_enhanced_drought_data(self, df: pd.DataFrame, data_dir: str) -> Tuple[np.ndarray, np.ndarray]:
        """Enhanced drought data preparation using real geographical and climatic features"""
        self.load_enhanced_datasets(data_dir)
        
        # Use ALL available geographical and topological features from drought data
        feature_cols = [
            'lat', 'lon', 'elevation', 'slope1', 'slope2', 'slope3', 'slope4', 'slope5', 
            'aspectN', 'aspectE', 'aspectS', 'aspectW', 'WAT_LAND', 'NVG_LAND', 
            'GRS_LAND', 'FOR_LAND', 'CULTRF_LAND', 'CULTIR_LAND'
        ]
        
        available_cols = [col for col in feature_cols if col in df.columns]
        
        # Merge with real climate data if available
        if self.agriculture_data is not None:
            # Create country mapping from coordinates (simplified)
            def get_country_from_coords(lat, lon):
                if 25 <= lat <= 49 and -125 <= lon <= -66:  # USA bounds
                    return 'USA'
                elif 8 <= lat <= 37 and 68 <= lon <= 97:     # India bounds
                    return 'India'
                elif 18 <= lat <= 54 and 73 <= lon <= 135:   # China bounds
                    return 'China'
                else:
                    return 'USA'  # Default
            
            enhanced_features = []
            for idx, row in df.iterrows():
                feature_row = []
                
                # Add base geographical features
                for col in available_cols:
                    feature_row.append(row[col] if col in row and pd.notna(row[col]) else 0)
                
                # Add climate features from real data
                country = get_country_from_coords(row.get('lat', 0), row.get('lon', 0))
                country_climate = self.agriculture_data[
                    self.agriculture_data['Country'] == country
                ]
                
                if not country_climate.empty:
                    # Use actual climate statistics
                    recent_data = country_climate[country_climate['Year'] >= 2015]
                    if not recent_data.empty:
                        feature_row.extend([
                            recent_data['Average_Temperature_C'].mean(),
                            recent_data['Total_Precipitation_mm'].mean(),
                            recent_data['CO2_Emissions_MT'].mean(),
                            recent_data['Extreme_Weather_Events'].mean(),
                            recent_data['Irrigation_Access_%'].mean(),
                            recent_data['Soil_Health_Index'].mean()
                        ])
                    else:
                        # Use overall country averages
                        feature_row.extend([
                            country_climate['Average_Temperature_C'].mean(),
                            country_climate['Total_Precipitation_mm'].mean(),
                            country_climate['CO2_Emissions_MT'].mean(),
                            country_climate['Extreme_Weather_Events'].mean(),
                            country_climate['Irrigation_Access_%'].mean(),
                            country_climate['Soil_Health_Index'].mean()
                        ])
                else:
                    # Use global averages as fallback
                    feature_row.extend([25.0, 800.0, 15.0, 5.0, 50.0, 70.0])
                
                enhanced_features.append(feature_row)
            
            X = np.array(enhanced_features)
            enhanced_cols = available_cols + [
                'avg_temperature', 'avg_precipitation', 'co2_emissions', 
                'extreme_weather_freq', 'irrigation_access', 'soil_health'
            ]
        else:
            X = df[available_cols].values
            enhanced_cols = available_cols
        
        # Use actual drought severity index (SQ1 is drought severity)
        if 'SQ1' in df.columns:
            # SQ1 appears to be drought severity: 1=severe, 2=moderate, 3=light
            # Convert to binary: 1-2 = drought (1), 3+ = no drought (0)
            y = (df['SQ1'] <= 2).astype(int).values
        else:
            y = np.random.randint(0, 2, len(df))
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Calculate data quality metrics
        self.data_quality_metrics['drought'] = {
            'completeness': np.mean(~np.isnan(X_scaled)),
            'feature_count': len(enhanced_cols),
            'sample_size': len(X_scaled),
            'positive_rate': np.mean(y),
            'data_source': 'real_geographical_climate'
        }
        
        self.scalers['drought'] = scaler
        self.feature_columns['drought'] = enhanced_cols
        
        return X_scaled, y

    def prepare_enhanced_flood_data(self, df: pd.DataFrame, data_dir: str) -> Tuple[np.ndarray, np.ndarray]:
        """Enhanced flood data preparation using real disaster and climate data"""
        self.load_enhanced_datasets(data_dir)
        
        # Use real flood disaster data
        df_flood = df[df['Entity'] == 'Flood'].copy() if 'Entity' in df.columns else df.copy()
        
        if len(df_flood) == 0:
            df_flood = df.copy()
        
        enhanced_features = []
        flood_targets = []
        
        for idx, row in df_flood.iterrows():
            year = int(row.get('Year', 2024))
            
            # Use actual climate data for the year and location
            if self.agriculture_data is not None:
                year_data = self.agriculture_data[self.agriculture_data['Year'] == year]
                if not year_data.empty:
                    # Use actual precipitation and extreme weather data
                    avg_precip = year_data['Total_Precipitation_mm'].mean()
                    extreme_weather = year_data['Extreme_Weather_Events'].mean()
                    avg_temp = year_data['Average_Temperature_C'].mean()
                    co2_level = year_data['CO2_Emissions_MT'].mean()
                else:
                    # Use recent years if specific year not available
                    recent_data = self.agriculture_data[self.agriculture_data['Year'] >= year - 3]
                    avg_precip = recent_data['Total_Precipitation_mm'].mean() if not recent_data.empty else 800.0
                    extreme_weather = recent_data['Extreme_Weather_Events'].mean() if not recent_data.empty else 5.0
                    avg_temp = recent_data['Average_Temperature_C'].mean() if not recent_data.empty else 25.0
                    co2_level = recent_data['CO2_Emissions_MT'].mean() if not recent_data.empty else 15.0
            else:
                # Fallback values
                avg_precip = 800.0
                extreme_weather = 5.0
                avg_temp = 25.0
                co2_level = 15.0
            
            # Create feature vector with real data
            feature_row = [
                year,
                (year - 2000) / 24.0,  # Normalized year
                avg_precip,
                extreme_weather,
                avg_temp,
                co2_level,
                avg_precip / 100.0,  # Precipitation intensity
                extreme_weather * avg_precip / 1000.0,  # Combined risk factor
                1 if year >= 2010 else 0,  # Climate change era
                np.sin(2 * np.pi * year / 11)  # Solar cycle
            ]
            enhanced_features.append(feature_row)
            
            # Use actual disaster counts or deaths as target
            if 'Disasters' in row:
                flood_intensity = row['Disasters']
                # Binary classification: high flood risk (>median) vs low
                flood_targets.append(flood_intensity)
            elif 'Deaths' in row:
                flood_intensity = row['Deaths']
                flood_targets.append(flood_intensity)
            else:
                # Use precipitation threshold as proxy
                flood_targets.append(avg_precip)
        
        X = np.array(enhanced_features)
        enhanced_cols = ['Year', 'Year_normalized', 'avg_precipitation', 'extreme_weather_freq',
                        'avg_temperature', 'co2_emissions', 'precip_intensity', 'risk_factor',
                        'climate_era', 'solar_cycle']
        
        # Convert to binary classification based on data distribution
        if len(flood_targets) > 0:
            flood_threshold = np.percentile(flood_targets, 70)  # Top 30% = high flood risk
            y = (np.array(flood_targets) > flood_threshold).astype(int)
        else:
            y = np.zeros(len(X))
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        self.data_quality_metrics['flood'] = {
            'completeness': np.mean(~np.isnan(X_scaled)),
            'feature_count': len(enhanced_cols),
            'sample_size': len(X_scaled),
            'positive_rate': np.mean(y),
            'data_source': 'real_disaster_climate'
        }
        
        self.scalers['flood'] = scaler
        self.feature_columns['flood'] = enhanced_cols
        
        return X_scaled, y

    def prepare_enhanced_hunger_data(self, df: pd.DataFrame, data_dir: str) -> Tuple[np.ndarray, np.ndarray]:
        """Enhanced hunger data preparation using real global hunger index and agricultural data"""
        self.load_enhanced_datasets(data_dir)
        
        # Use actual Global Hunger Index data
        df_clean = df.dropna(subset=['Global Hunger Index (2021)']) if 'Global Hunger Index (2021)' in df.columns else df
        
        if len(df_clean) == 0:
            df_clean = df.copy()
        
        enhanced_features = []
        hunger_targets = []
        
        for idx, row in df_clean.iterrows():
            year = int(row.get('Year', 2024))
            country = row.get('Entity', 'Unknown')
            
            # Get real agricultural and climate data for the country
            if self.agriculture_data is not None:
                country_data = self.agriculture_data[
                    self.agriculture_data['Country'].str.contains(country, case=False, na=False)
                ]
                
                if not country_data.empty:
                    # Use actual country-specific data
                    recent_data = country_data[country_data['Year'] >= year - 3]
                    if not recent_data.empty:
                        crop_yield = recent_data['Crop_Yield_MT_per_HA'].mean()
                        avg_temp = recent_data['Average_Temperature_C'].mean()
                        precipitation = recent_data['Total_Precipitation_mm'].mean()
                        extreme_weather = recent_data['Extreme_Weather_Events'].mean()
                        irrigation_access = recent_data['Irrigation_Access_%'].mean()
                        soil_health = recent_data['Soil_Health_Index'].mean()
                        economic_impact = recent_data['Economic_Impact_Million_USD'].mean()
                    else:
                        # Use overall country averages
                        crop_yield = country_data['Crop_Yield_MT_per_HA'].mean()
                        avg_temp = country_data['Average_Temperature_C'].mean()
                        precipitation = country_data['Total_Precipitation_mm'].mean()
                        extreme_weather = country_data['Extreme_Weather_Events'].mean()
                        irrigation_access = country_data['Irrigation_Access_%'].mean()
                        soil_health = country_data['Soil_Health_Index'].mean()
                        economic_impact = country_data['Economic_Impact_Million_USD'].mean()
                else:
                    # Use global averages
                    crop_yield = self.agriculture_data['Crop_Yield_MT_per_HA'].mean()
                    avg_temp = self.agriculture_data['Average_Temperature_C'].mean()
                    precipitation = self.agriculture_data['Total_Precipitation_mm'].mean()
                    extreme_weather = self.agriculture_data['Extreme_Weather_Events'].mean()
                    irrigation_access = self.agriculture_data['Irrigation_Access_%'].mean()
                    soil_health = self.agriculture_data['Soil_Health_Index'].mean()
                    economic_impact = self.agriculture_data['Economic_Impact_Million_USD'].mean()
            else:
                # Fallback values
                crop_yield = 2.0
                avg_temp = 25.0
                precipitation = 800.0
                extreme_weather = 5.0
                irrigation_access = 50.0
                soil_health = 70.0
                economic_impact = 500.0
            
            # Create comprehensive feature vector
            feature_row = [
                year,
                (year - 2000) / 24.0,  # Normalized year
                crop_yield,
                avg_temp,
                precipitation,
                extreme_weather,
                irrigation_access,
                soil_health,
                economic_impact / 1000.0,  # Normalized economic impact
                crop_yield / avg_temp,  # Yield-temperature ratio
                precipitation / 100.0,  # Precipitation index
                1 if irrigation_access > 50 else 0,  # Good irrigation access
                1 if soil_health > 70 else 0,  # Good soil health
                extreme_weather * (1 / max(crop_yield, 0.1))  # Risk factor
            ]
            enhanced_features.append(feature_row)
            
            # Use actual Global Hunger Index as target
            if 'Global Hunger Index (2021)' in row and pd.notna(row['Global Hunger Index (2021)']):
                ghi_value = row['Global Hunger Index (2021)']
                hunger_targets.append(ghi_value)
            else:
                # Estimate based on agricultural factors
                estimated_ghi = max(0, 50 - (crop_yield * 10) + (extreme_weather * 2))
                hunger_targets.append(estimated_ghi)
        
        X = np.array(enhanced_features)
        enhanced_cols = ['Year', 'Year_normalized', 'crop_yield_avg', 'avg_temperature',
                        'avg_precipitation', 'extreme_weather_freq', 'irrigation_access', 
                        'soil_health', 'economic_impact', 'yield_temp_ratio', 'precip_index',
                        'good_irrigation', 'good_soil', 'risk_factor']
        
        # Convert to hunger severity classification based on GHI standards
        if len(hunger_targets) > 0:
            hunger_values = np.array(hunger_targets)
            # GHI classification: <9.9=low, 10-19.9=moderate, 20-34.9=serious, 35+=alarming
            y = np.where(hunger_values < 9.9, 0, 
                        np.where(hunger_values < 19.9, 1, 
                                np.where(hunger_values < 34.9, 2, 3)))
        else:
            y = np.zeros(len(X))
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        self.data_quality_metrics['hunger'] = {
            'completeness': np.mean(~np.isnan(X_scaled)),
            'feature_count': len(enhanced_cols),
            'sample_size': len(X_scaled),
            'class_distribution': np.bincount(y).tolist(),
            'data_source': 'real_ghi_agricultural'
        }
        
        self.scalers['hunger'] = scaler
        self.feature_columns['hunger'] = enhanced_cols
        
        return X_scaled, y

    def prepare_enhanced_crop_data(self, df: pd.DataFrame, data_dir: str) -> Tuple[np.ndarray, np.ndarray]:
        """Enhanced crop data preparation using real agricultural features"""
        self.load_enhanced_datasets(data_dir)
        
        # Check if we have the dedicated crop yield dataset
        crop_yield_path = os.path.join(data_dir, 'crop_yield_data.csv')
        if os.path.exists(crop_yield_path):
            crop_df = pd.read_csv(crop_yield_path)
            
            # Use real features from crop yield dataset
            feature_cols = ['rainfall_mm', 'soil_quality_index', 'farm_size_hectares', 
                           'sunlight_hours', 'fertilizer_kg']
            
            X = crop_df[feature_cols].values
            y = crop_df['crop_yield'].values
            
            # Convert to tonnes per hectare (divide by 100 to normalize)
            y = y / 100.0
            
            enhanced_cols = feature_cols
            
        else:
            # Use agriculture climate data as fallback
            enhanced_features = []
            yields = []
            
            for idx, row in df.iterrows():
                # Use actual climate data features
                feature_row = [
                    row.get('Year', 2024),
                    row.get('Average_Temperature_C', 25.0),
                    row.get('Total_Precipitation_mm', 800.0),
                    row.get('CO2_Emissions_MT', 15.0),
                    row.get('Irrigation_Access_%', 50.0),
                    row.get('Soil_Health_Index', 70.0),
                    row.get('Extreme_Weather_Events', 5.0),
                    row.get('Fertilizer_Use_KG_per_HA', 100.0),
                    row.get('Pesticide_Use_KG_per_HA', 20.0)
                ]
                enhanced_features.append(feature_row)
                
                # Use actual crop yield data
                actual_yield = row.get('Crop_Yield_MT_per_HA', 2.0)
                yields.append(max(0.1, actual_yield))  # Ensure positive yield
            
            X = np.array(enhanced_features)
            y = np.array(yields)
            
            enhanced_cols = ['Year', 'avg_temperature', 'avg_precipitation', 'co2_emissions',
                            'irrigation_access', 'soil_health', 'extreme_weather_freq', 
                            'fertilizer_use', 'pesticide_use']
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        self.data_quality_metrics['crop'] = {
            'completeness': np.mean(~np.isnan(X_scaled)),
            'feature_count': len(enhanced_cols),
            'sample_size': len(X_scaled),
            'yield_range': f"{y.min():.2f}-{y.max():.2f} MT/HA",
            'data_source': 'real_agricultural_data'
        }
        
        self.scalers['crop'] = scaler
        self.feature_columns['crop'] = enhanced_cols
        
        return X_scaled, y
    
    def save_preprocessors(self, model_dir: str):
        os.makedirs(model_dir, exist_ok=True)
        
        with open(os.path.join(model_dir, 'scalers.pkl'), 'wb') as f:
            pickle.dump(self.scalers, f)
        
        with open(os.path.join(model_dir, 'feature_columns.pkl'), 'wb') as f:
            pickle.dump(self.feature_columns, f)
    
    def load_preprocessors(self, model_dir: str):
        with open(os.path.join(model_dir, 'scalers.pkl'), 'rb') as f:
            self.scalers = pickle.load(f)
        
        with open(os.path.join(model_dir, 'feature_columns.pkl'), 'rb') as f:
            self.feature_columns = pickle.load(f)
    
    def add_climate_features(self, df: pd.DataFrame, location_data: Dict = None) -> pd.DataFrame:
        """Add climate zone and seasonal features for better predictions"""
        if location_data and 'lat' in location_data:
            # Climate zone classification based on latitude
            lat = location_data['lat']
            if abs(lat) < 23.5:
                df['climate_zone'] = 0  # Tropical
            elif abs(lat) < 35:
                df['climate_zone'] = 1  # Subtropical
            elif abs(lat) < 50:
                df['climate_zone'] = 2  # Temperate
            else:
                df['climate_zone'] = 3  # Cold
        else:
            df['climate_zone'] = 0  # Default to tropical for Africa
        
        # Add seasonal patterns if Year exists
        if 'Year' in df.columns:
            # Cyclical encoding for years to capture climate cycles
            df['year_sin'] = np.sin(2 * np.pi * df['Year'] / 11)  # Solar cycle
            df['year_cos'] = np.cos(2 * np.pi * df['Year'] / 11)
            
            # El Ni침o/La Ni침a cycle approximation
            df['enso_cycle'] = np.sin(2 * np.pi * df['Year'] / 3.5)
        
        return df

class ModelUtils:
    @staticmethod
    def save_model(model: Any, model_path: str):
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
    
    @staticmethod
    def load_model(model_path: str) -> Any:
        with open(model_path, 'rb') as f:
            return pickle.load(f)
    
    @staticmethod
    def evaluate_model_performance(y_true: np.ndarray, y_pred: np.ndarray, model_type: str) -> Dict:
        if model_type == 'regression':
            from sklearn.metrics import mean_squared_error, r2_score
            mse = mean_squared_error(y_true, y_pred)
            r2 = r2_score(y_true, y_pred)
            return {'mse': mse, 'r2': r2, 'rmse': np.sqrt(mse)}
        else:
            from sklearn.metrics import accuracy_score, classification_report
            accuracy = accuracy_score(y_true, y_pred)
            return {'accuracy': accuracy}
    
    @staticmethod
    def create_prediction_input(location_data: Dict, hazard_type: str, 
                              feature_columns: List[str]) -> np.ndarray:
        """Create prediction input using real feature mappings"""
        feature_values = []
        
        # Get actual coordinates and derive features
        lat = location_data.get('lat', 0.0)
        lon = location_data.get('lon', 0.0)
        temp = location_data.get('temperature', 25.0)
        humidity = location_data.get('humidity', 65.0)
        precipitation = location_data.get('precipitation', 800.0)
        
        if hazard_type == 'drought':
            # Use realistic geographical and climate mappings
            defaults = {
                'lat': lat,
                'lon': lon,
                'elevation': abs(lat) * 50 + 100,  # Elevation estimate from latitude
                'slope1': 0.1, 'slope2': 0.15, 'slope3': 0.2, 'slope4': 0.25, 'slope5': 0.1,
                'aspectN': 0.25, 'aspectE': 0.25, 'aspectS': 0.25, 'aspectW': 0.25,
                'WAT_LAND': max(0, 10 - abs(lat) * 0.2),  # Water availability decreases with latitude
                'NVG_LAND': 15.0, 'GRS_LAND': 40.0, 'FOR_LAND': 25.0, 
                'CULTRF_LAND': 20.0, 'CULTIR_LAND': 15.0,
                'avg_temperature': temp,
                'avg_precipitation': precipitation,
                'co2_emissions': 15.0,
                'extreme_weather_freq': 5.0,
                'irrigation_access': 50.0,
                'soil_health': 70.0
            }
        elif hazard_type == 'flood':
            defaults = {
                'Year': 2025,
                'Year_normalized': 1.0,
                'avg_precipitation': precipitation,
                'extreme_weather_freq': precipitation / 200.0,  # Higher precip = more extreme weather
                'avg_temperature': temp,
                'co2_emissions': 15.0,
                'precip_intensity': precipitation / 100.0,
                'risk_factor': (precipitation / 200.0) * (precipitation / 1000.0),
                'climate_era': 1,
                'solar_cycle': 0.5
            }
        elif hazard_type == 'hunger':
            # Estimate agricultural factors from climate
            estimated_yield = max(0.5, 3.0 - (temp - 25) * 0.1 + (precipitation - 800) * 0.001)
            defaults = {
                'Year': 2025,
                'Year_normalized': 1.0,
                'crop_yield_avg': estimated_yield,
                'avg_temperature': temp,
                'avg_precipitation': precipitation,
                'extreme_weather_freq': abs(temp - 25) * 0.5 + abs(precipitation - 800) * 0.01,
                'irrigation_access': 50.0,
                'soil_health': 70.0,
                'economic_impact': 0.5,
                'yield_temp_ratio': estimated_yield / temp,
                'precip_index': precipitation / 100.0,
                'good_irrigation': 1 if 50.0 > 50 else 0,
                'good_soil': 1 if 70.0 > 70 else 0,
                'risk_factor': (abs(temp - 25) * 0.5) * (1 / max(estimated_yield, 0.1))
            }
        elif hazard_type == 'crop':
            # Check if using dedicated crop yield features
            if 'rainfall_mm' in feature_columns:
                defaults = {
                    'rainfall_mm': precipitation,
                    'soil_quality_index': 7,  # Scale 1-10
                    'farm_size_hectares': 100,
                    'sunlight_hours': max(4, 12 - abs(lat) * 0.1),
                    'fertilizer_kg': 200
                }
            else:
                defaults = {
                    'Year': 2025,
                    'avg_temperature': temp,
                    'avg_precipitation': precipitation,
                    'co2_emissions': 15.0,
                    'irrigation_access': 50.0,
                    'soil_health': 70.0,
                    'extreme_weather_freq': 5.0,
                    'fertilizer_use': 100.0,
                    'pesticide_use': 20.0
                }
        else:
            # Generic defaults
            defaults = {
                'temperature': temp,
                'precipitation': precipitation,
                'humidity': humidity,
                'latitude': lat,
                'longitude': lon
            }
        
        # Fill feature values based on actual column names
        for col in feature_columns:
            if col in defaults:
                feature_values.append(defaults[col])
            else:
                # Try to map common variations
                if 'temp' in col.lower():
                    feature_values.append(temp)
                elif 'precip' in col.lower() or 'rain' in col.lower():
                    feature_values.append(precipitation)
                elif 'humid' in col.lower():
                    feature_values.append(humidity)
                elif 'lat' in col.lower():
                    feature_values.append(lat)
                elif 'lon' in col.lower():
                    feature_values.append(lon)
                else:
                    feature_values.append(0.0)
        
        return np.array(feature_values).reshape(1, -1)

class ResponseFormatter:
    @staticmethod
    def _get_confidence_indicator(confidence: float) -> str:
        """Return indicator based on confidence level"""
        if confidence >= 85:
            return "游릭"  # High confidence
        elif confidence >= 70:
            return "游리"  # Medium confidence
        elif confidence >= 50:
            return "游댮"  # Low confidence
        else:
            return "游릯"  # Poor confidence
    
    @staticmethod
    def _add_actionable_advice(risk_level: str, hazard_type: str) -> str:
        """Add actionable advice based on prediction"""
        advice_map = {
            "drought": {
                "High": "Recommendation: Increase water storage, consider drought-resistant crops",
                "Low": "Recommendation: Normal farming practices, monitor seasonal forecasts"
            },
            "flood": {
                "High": "Recommendation: Prepare drainage systems, consider elevated storage",
                "Low": "Recommendation: Standard precautions, maintain drainage infrastructure"
            },
            "hunger": {
                "High": "Recommendation: Enhance food security programs, diversify income sources",
                "Moderate": "Recommendation: Monitor food prices, strengthen community networks",
                "Low": "Recommendation: Maintain current nutrition programs"
            }
        }
        return advice_map.get(hazard_type, {}).get(risk_level, "")
    
    @staticmethod
    def format_drought_prediction(prediction: int, confidence: float, location: str) -> str:
        risk_level = "High" if prediction == 1 else "Low"
        confidence_icon = ResponseFormatter._get_confidence_indicator(confidence)
        advice = ResponseFormatter._add_actionable_advice(risk_level, "drought")
        return f"Drought risk in {location.title()}: {risk_level} {confidence_icon} (Confidence: {confidence:.1f}%)\n{advice}"
    
    @staticmethod
    def format_flood_prediction(prediction: int, confidence: float, location: str) -> str:
        risk_level = "High" if prediction == 1 else "Low"
        confidence_icon = ResponseFormatter._get_confidence_indicator(confidence)
        advice = ResponseFormatter._add_actionable_advice(risk_level, "flood")
        return f"Flood risk in {location.title()}: {risk_level} {confidence_icon} (Confidence: {confidence:.1f}%)\n{advice}"
    
    @staticmethod
    def format_hunger_prediction(prediction: int, confidence: float, location: str) -> str:
        risk_levels = {0: "Low", 1: "Moderate", 2: "High"}
        risk_level = risk_levels.get(prediction, "Unknown")
        confidence_icon = ResponseFormatter._get_confidence_indicator(confidence)
        advice = ResponseFormatter._add_actionable_advice(risk_level, "hunger")
        return f"Hunger risk in {location.title()}: {risk_level} {confidence_icon} (Confidence: {confidence:.1f}%)\n{advice}"
    
    @staticmethod
    def format_crop_prediction(prediction: float, confidence: float, location: str) -> str:
        confidence_icon = ResponseFormatter._get_confidence_indicator(confidence)
        
        # Categorize yield levels
        if prediction >= 4.0:
            yield_category = "Excellent"
            color_indicator = "游릭"
        elif prediction >= 2.5:
            yield_category = "Good"
            color_indicator = "游리"
        elif prediction >= 1.5:
            yield_category = "Fair"
            color_indicator = "游댮"
        else:
            yield_category = "Poor"
            color_indicator = "游릯"
        
        return f"Expected crop yield in {location.title()}: {prediction:.2f} MT/HA ({yield_category} {color_indicator}) {confidence_icon} (Confidence: {confidence:.1f}%)"
    
    @staticmethod
    def create_risk_summary_table(location: str, predictions: Dict) -> str:
        """Create a formatted summary table of all risks for a location"""
        summary = f"**Risk Summary for {location.title()}**\n\n"
        summary += "| Risk Type | Level | Confidence | Status |\n"
        summary += "|:---------:|:-----:|:----------:|:------:|\n"
        
        for hazard_type, result in predictions.items():
            if result and not result.get('error'):
                pred = result['prediction']
                conf = result['confidence']
                
                if hazard_type == 'crop':
                    level = "Good" if pred >= 2.5 else "Poor"
                elif hazard_type == 'hunger':
                    levels = {0: "Low", 1: "Moderate", 2: "High"}
                    level = levels.get(pred, "Unknown")
                else:
                    level = "High" if pred == 1 else "Low"
                
                status_icon = ResponseFormatter._get_confidence_indicator(conf)
                summary += f"| **{hazard_type.title()}** | {level} | {conf:.1f}% | {status_icon} |\n"
        
        # Add confidence explanation
        summary += "\n**Confidence Level Guide:**\n"
        summary += "- 游릭 High (85%+): Very reliable prediction\n"
        summary += "- 游리 Medium (70-84%): Moderately reliable prediction\n"
        summary += "- 游댮 Low (50-69%): Less reliable, use with caution\n"
        summary += "- 游릯 Poor (<50%): Very low reliability, seek additional data\n"
        
        return summary

    # ...existing code...
def validate_data_files(data_dir: str) -> Dict[str, bool]:
    required_files = [
        'drought_data.csv',
        'flood_data.csv', 
        'hunger_data.csv',
        'crop_data.csv'
    ]
    
    file_status = {}
    for file in required_files:
        file_path = os.path.join(data_dir, file)
        file_status[file] = os.path.exists(file_path)
    
    return file_status

def get_model_paths(models_dir: str) -> Dict[str, str]:
    return {
        'drought': os.path.join(models_dir, 'drought_model.pkl'),
        'flood': os.path.join(models_dir, 'flood_model.pkl'),
        'hunger': os.path.join(models_dir, 'hunger_model.pkl'),
        'crop': os.path.join(models_dir, 'crop_yield_model.pkl')
    }
